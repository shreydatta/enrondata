{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import pandas as pd\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "# features_list = ['poi','salary'] # You will need to use more features\n",
    "### Load the dictionary containing the dataset\n",
    "# with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "#     data_dict = pickle.load(data_file)\n",
    "def load_data_as_df():\n",
    "    with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "        data_dict = pickle.load(data_file)\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df = df.transpose()\n",
    "    return df\n",
    "\n",
    "df=load_data_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have total 146 data points. As in, the data of 146 different individuals supposedly working with Enron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['poi']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these 146 we have 18 are marked as POI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>email_address</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>...</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>4175000</td>\n",
       "      <td>2869717</td>\n",
       "      <td>-3081055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>1729541</td>\n",
       "      <td>13868</td>\n",
       "      <td>2195</td>\n",
       "      <td>47</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>304805</td>\n",
       "      <td>152</td>\n",
       "      <td>False</td>\n",
       "      <td>126027</td>\n",
       "      <td>-126027</td>\n",
       "      <td>201955</td>\n",
       "      <td>1407</td>\n",
       "      <td>2902</td>\n",
       "      <td>4484442</td>\n",
       "      <td>1729541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182466</td>\n",
       "      <td>257817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>james.bannantine@enron.com</td>\n",
       "      <td>4046157</td>\n",
       "      <td>56301</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864523</td>\n",
       "      <td>False</td>\n",
       "      <td>1757552</td>\n",
       "      <td>-560222</td>\n",
       "      <td>477</td>\n",
       "      <td>465</td>\n",
       "      <td>566</td>\n",
       "      <td>916197</td>\n",
       "      <td>5243487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>False</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>400000</td>\n",
       "      <td>260455</td>\n",
       "      <td>-201641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frank.bay@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "      <td>145796</td>\n",
       "      <td>-82782</td>\n",
       "      <td>239671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827696</td>\n",
       "      <td>63014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bonus deferral_payments deferred_income director_fees  \\\n",
       "ALLEN PHILLIP K     4175000           2869717        -3081055           NaN   \n",
       "BADUM JAMES P           NaN            178980             NaN           NaN   \n",
       "BANNANTINE JAMES M      NaN               NaN           -5104           NaN   \n",
       "BAXTER JOHN C       1200000           1295738        -1386055           NaN   \n",
       "BAY FRANKLIN R       400000            260455         -201641           NaN   \n",
       "\n",
       "                                 email_address exercised_stock_options  \\\n",
       "ALLEN PHILLIP K        phillip.allen@enron.com                 1729541   \n",
       "BADUM JAMES P                              NaN                  257817   \n",
       "BANNANTINE JAMES M  james.bannantine@enron.com                 4046157   \n",
       "BAXTER JOHN C                              NaN                 6680544   \n",
       "BAY FRANKLIN R             frank.bay@enron.com                     NaN   \n",
       "\n",
       "                   expenses from_messages from_poi_to_this_person  \\\n",
       "ALLEN PHILLIP K       13868          2195                      47   \n",
       "BADUM JAMES P          3486           NaN                     NaN   \n",
       "BANNANTINE JAMES M    56301            29                      39   \n",
       "BAXTER JOHN C         11200           NaN                     NaN   \n",
       "BAY FRANKLIN R       129142           NaN                     NaN   \n",
       "\n",
       "                   from_this_person_to_poi        ...         \\\n",
       "ALLEN PHILLIP K                         65        ...          \n",
       "BADUM JAMES P                          NaN        ...          \n",
       "BANNANTINE JAMES M                       0        ...          \n",
       "BAXTER JOHN C                          NaN        ...          \n",
       "BAY FRANKLIN R                         NaN        ...          \n",
       "\n",
       "                   long_term_incentive    other    poi restricted_stock  \\\n",
       "ALLEN PHILLIP K                 304805      152  False           126027   \n",
       "BADUM JAMES P                      NaN      NaN  False              NaN   \n",
       "BANNANTINE JAMES M                 NaN   864523  False          1757552   \n",
       "BAXTER JOHN C                  1586055  2660303  False          3942714   \n",
       "BAY FRANKLIN R                     NaN       69  False           145796   \n",
       "\n",
       "                   restricted_stock_deferred  salary shared_receipt_with_poi  \\\n",
       "ALLEN PHILLIP K                      -126027  201955                    1407   \n",
       "BADUM JAMES P                            NaN     NaN                     NaN   \n",
       "BANNANTINE JAMES M                   -560222     477                     465   \n",
       "BAXTER JOHN C                            NaN  267102                     NaN   \n",
       "BAY FRANKLIN R                        -82782  239671                     NaN   \n",
       "\n",
       "                   to_messages total_payments total_stock_value  \n",
       "ALLEN PHILLIP K           2902        4484442           1729541  \n",
       "BADUM JAMES P              NaN         182466            257817  \n",
       "BANNANTINE JAMES M         566         916197           5243487  \n",
       "BAXTER JOHN C              NaN        5634343          10623258  \n",
       "BAY FRANKLIN R             NaN         827696             63014  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the emai_address feature and convert every other values as float values. To, get the statistics and correlation easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('email_address', axis=1)\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                         64\n",
       "deferral_payments            107\n",
       "deferred_income               97\n",
       "director_fees                129\n",
       "exercised_stock_options       44\n",
       "expenses                      51\n",
       "from_messages                 60\n",
       "from_poi_to_this_person       60\n",
       "from_this_person_to_poi       60\n",
       "loan_advances                142\n",
       "long_term_incentive           80\n",
       "other                         53\n",
       "poi                            0\n",
       "restricted_stock              36\n",
       "restricted_stock_deferred    128\n",
       "salary                        51\n",
       "shared_receipt_with_poi       60\n",
       "to_messages                   60\n",
       "total_payments                21\n",
       "total_stock_value             20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the features having more than 90 null values.We can remove those features as we will not be getting much information from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deferral_payments\n",
      "deferred_income\n",
      "director_fees\n",
      "loan_advances\n",
      "restricted_stock_deferred\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns: \n",
    "    if df[i].isnull().sum() > 90:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"loan_advances\", axis=1)\n",
    "df = df.drop('restricted_stock_deferred', axis=1)\n",
    "df = df.drop('director_fees', axis=1)\n",
    "df = df.drop('deferral_payments', axis=1)\n",
    "df = df.drop('deferred_income', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                     -0.013837\n",
       "exercised_stock_options    0.052886\n",
       "expenses                  -0.044508\n",
       "from_messages             -0.074308\n",
       "from_poi_to_this_person    0.167722\n",
       "from_this_person_to_poi    0.112940\n",
       "long_term_incentive       -0.021222\n",
       "other                     -0.012457\n",
       "poi                        1.000000\n",
       "restricted_stock          -0.000107\n",
       "salary                    -0.030884\n",
       "shared_receipt_with_poi    0.228313\n",
       "to_messages                0.058954\n",
       "total_payments             0.040130\n",
       "total_stock_value          0.025163\n",
       "Name: poi, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since correlation value for restricted_stock is very low we will drop that\n",
    "df = df.drop('restricted_stock', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the rows now. Rows having less than 3 feature values will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "for i in df.index:\n",
    "        if df.ix[i].count() < 3:\n",
    "            df = df.drop(i, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 indexes named as TOTAL & THE TRAVEL AGENCY IN THE PARK which cannot be person working with or for the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.drop('TOTAL',axis=0)\n",
    "df=df.drop('THE TRAVEL AGENCY IN THE PARK',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring new features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the patterns for the ratio of messages coming from the poi to this person to the total no of to messages, and we will be doing the same for messages sent. Having a large no. of portions of emails sent by person is to a poi then likely that person is POI, and same goes for emails received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "df['pct_from_poi'] = df['from_poi_to_this_person']/(df['to_messages'] + 1)\n",
    "df['pct_to_poi'] = df['from_this_person_to_poi']/(df['from_messages'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = list(df.columns)\n",
    "features_list.remove('poi')\n",
    "features = df[features_list]\n",
    "labels = df['poi']\n",
    "features_list.insert(0,'poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'bonus', 'exercised_stock_options', 'expenses', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi', 'long_term_incentive', 'other', 'salary', 'shared_receipt_with_poi', 'to_messages', 'total_payments', 'total_stock_value', 'pct_from_poi', 'pct_to_poi']\n"
     ]
    }
   ],
   "source": [
    "print features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "df1 = df.transpose()\n",
    "df1 = df1.to_dict()\n",
    "my_dataset = df1\n",
    "\n",
    "# ### Extract features and labels from dataset for local testing\n",
    "# data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "# labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.htm\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "clf1 = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN',strategy='median')),\n",
    "        ('clf', GaussianNB())\n",
    "    ])\n",
    "clf2 = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN',strategy='median')),\n",
    "        ('clf', SVC())\n",
    "    ])\n",
    "clf3 = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN',strategy='median')),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "clf4 = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN',strategy='median')),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "clf5 = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN',strategy='median')),\n",
    "        ('clf', AdaBoostClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', GaussianNB())])\n",
      "\tAccuracy: 0.82350\tPrecision: 0.30521\tRecall: 0.18450\tF1: 0.22998\tF2: 0.20035\n",
      "\tTotal predictions: 14000\tTrue positives:  369\tFalse positives:  840\tFalse negatives: 1631\tTrue negatives: 11160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf1, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf2, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.78957\tPrecision: 0.25492\tRecall: 0.24600\tF1: 0.25038\tF2: 0.24773\n",
      "\tTotal predictions: 14000\tTrue positives:  492\tFalse positives: 1438\tFalse negatives: 1508\tTrue negatives: 10562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf3, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\tAccuracy: 0.84436\tPrecision: 0.36896\tRecall: 0.12600\tF1: 0.18785\tF2: 0.14511\n",
      "\tTotal predictions: 14000\tTrue positives:  252\tFalse positives:  431\tFalse negatives: 1748\tTrue negatives: 11569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf4, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))])\n",
      "\tAccuracy: 0.81393\tPrecision: 0.29037\tRecall: 0.20950\tF1: 0.24339\tF2: 0.22186\n",
      "\tTotal predictions: 14000\tTrue positives:  419\tFalse positives: 1024\tFalse negatives: 1581\tTrue negatives: 10976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf5, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take clf3 i.e DecisionTreeClassifier as it has the highest f1 score, and a good balance b/w precision and recall scores of all the algorithms then tune it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection (without PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a look at the DecisionTreeClassifier feature importance attributes. \n",
    "\n",
    "Also, we will create a new feature_list where we will include all the features having feature importances > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bonus', 0.0)\n",
      "('exercised_stock_options', 0.10751146788990827)\n",
      "('expenses', 0.0)\n",
      "('from_messages', 0.0)\n",
      "('from_poi_to_this_person', 0.080949811117107459)\n",
      "('from_this_person_to_poi', 0.0)\n",
      "('long_term_incentive', 0.023067594643045305)\n",
      "('other', 0.11467889908256881)\n",
      "('salary', 0.047782874617737003)\n",
      "('shared_receipt_with_poi', 0.1747487986020097)\n",
      "('to_messages', 0.0)\n",
      "('total_payments', 0.0)\n",
      "('total_stock_value', 0.089222160708636561)\n",
      "('pct_from_poi', 0.15557541824069068)\n",
      "('pct_to_poi', 0.20646297509829625)\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "for name, importance in zip(features.columns, clf3.named_steps['clf'].feature_importances_):\n",
    "    print(name, importance)\n",
    "    if importance > 0:\n",
    "        features_list.append(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have selected the features which feature importances score greater than 0. I don't see the need for using PCA over here as we have very less features available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the new features according to the new features_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=features[features_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As, shown by the feature importance score one of the new features engineered i.e \"from_poi_to_this_person\" into the feature list is performing above the threshold value i.e 0.\n",
    "\n",
    "So, the modified feature list will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list.insert(0,'poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'exercised_stock_options',\n",
       " 'from_poi_to_this_person',\n",
       " 'long_term_incentive',\n",
       " 'other',\n",
       " 'salary',\n",
       " 'shared_receipt_with_poi',\n",
       " 'total_stock_value',\n",
       " 'pct_from_poi',\n",
       " 'pct_to_poi']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since DecisionTreeClassifier doesn't require scaling because these classifiers don't rely on the Euclidean distance between data points when making decisions. \n",
    "\n",
    "So, there is no feature scaling done to the new features created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the new data set to be passed to test_classifier using our new features_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2=df[features_list]\n",
    "df1 = df2.transpose()\n",
    "df1 = df1.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To justify Validation\n",
    "\n",
    "Let's train our model on the entire data set, and see the metrics of our model on the Training data . We expect the metrics of our model to be good on the training features as it is trained on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "clf3.fit(features, labels)\n",
    "pred = clf3.predict(features)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print accuracy_score(pred,labels)\n",
    "print precision_score(pred,labels)\n",
    "print recall_score(pred,labels)\n",
    "print f1_score(pred,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our metrics comes so perfect.\n",
    "\n",
    "\n",
    "Let's see metrics of our model on unseen data. \n",
    "\n",
    "\n",
    "We expect our classifier to do well as it is trained on the large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.79543\tPrecision: 0.24648\tRecall: 0.21000\tF1: 0.22678\tF2: 0.21641\n",
      "\tTotal predictions: 14000\tTrue positives:  420\tFalse positives: 1284\tFalse negatives: 1580\tTrue negatives: 10716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf3, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics of our model differs alot on unseen data.\n",
    "\n",
    "It was perfect previously because the model was trained and tested on the same data set which gave us the wrong idea about the model metrics. That's why it is necessary to perform validation.\n",
    "\n",
    "Since, the proportion of population of POI's is very low . So, we call it as a imbalanced classification problem.\n",
    "\n",
    "Due to the class imbalance problem, it is preferred to use a stratified shuffle split instead. This ensures that an equal ratio of POIs to non-POIs are found in the training and test sets.\n",
    "\n",
    "As this is a imbalanced classification problem .We need to have a good precision and recall score to proove that the model is performing well. \n",
    "\n",
    "In context of this project, \n",
    "\n",
    "recall =    TP/(TP + FN)      =    POI's correctly identified/(POI's correctly identified + POI's incorrectly labelled as Non-POI's)\n",
    "\n",
    "precision = TP/(TP + FP)     =     POI's correctly identified/(POI's correctly identified + Non POI's incorrectly labelled as POI's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Howerver, We can see that on just removing the non important features our model metrics goes higher as compared to the previous run on all the features_list.\n",
    "\n",
    "Where it was -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.79336\tPrecision: 0.23995\tRecall: 0.20600\tF1: 0.22168\tF2: 0.21200\n",
      "\tTotal predictions: 14000\tTrue positives:  412\tFalse positives: 1305\tFalse negatives: 1588\tTrue negatives: 10695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf3, df1, features_list, folds = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of parameters to be passed to DecisionTreeClassifier. These parameters are kind of knobs which decides the performance of the classifier. \n",
    "\n",
    "\n",
    "While training the model these parameters are not learnt.\n",
    "\n",
    "\n",
    "Using Grid Search with parameter grid will train model on the various parameters combinations and training and tested the data splitted by StratifiedShuffleSplit. Using the scorer passed to GridSearch we have created a custom scorer to maximize recall score coming by the different model combinations. The model with the highest recall score will be selected and we will be using it's parameters to create the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning DecisionTreeClassifier / Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n",
    "\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning is essentially selecting the best parameters for an algorithm to optimize its performance given a working environment such as hardware, specific workloads, etc. And tuning in machine learning is an automated process for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split,StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(labels, 3, test_size=0.3, random_state=0)\n",
    "    \n",
    "for train_index, test_index in sss:\n",
    "    features_train = features.iloc[train_index]\n",
    "    features_test= features.iloc[test_index]\n",
    "    labels_train, labels_test = labels.iloc[train_index], labels.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection by SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be deploying SelectKBest in our pipeline to further have the best features selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "Pipeline2 = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN')),\n",
    "        ('kbest', SelectKBest(f_classif)),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be passing [3,4,5,6,7,8,9] values for k parameter of SelectKBest so that gridsearch can try all the combinations with these values and give us the best parameter value of K which should be used in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "score = make_scorer(recall_score, greater_is_better=True)\n",
    "# Build Grid\n",
    "# pre-processing\n",
    "# c = [x for x in range(5,8)]\n",
    "# c=[3,4,5]\n",
    "# estimator parameters\n",
    "e = [100,200,300,400,500,600,700]\n",
    "r = [0.1,0.2,0.3,0.4,0.5,0.6,0.7]\n",
    "c = [d for d in range(3, 7)]\n",
    "\n",
    "param_grid2 = {'imp__strategy': ['mean','median','most_frequent'],\n",
    "               'kbest__k': [3,4,5,6,7,8,9],\n",
    "              'clf__criterion': ['gini','entropy'],\n",
    "              'clf__max_depth': [3,4,5,6,7,8,9],\n",
    "              'clf__presort': [True,False],\n",
    "               'clf__max_features': [\"auto\",\"sqrt\",\"log2\"]\n",
    "              \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be tuning the strategies for Imputer 'mean','median','most_frequent'.\n",
    "\n",
    "Also, we will be tuning below mentioned parameters of DecisionTreeClassifier -\n",
    "\n",
    "    criterion - possiible values as 'gini','entropy'\n",
    "    max_depth - possible values as 3,4,5,6,7,8,9\n",
    "    presort - possible values as True, False\n",
    "    max_features - possible values as \"auto\",\"sqrt\",\"log2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation definition and importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using StratifiedShuffleSplit for cross_validation in gridsearch.It will take 25% data set as test set and train the model on the rest 75% data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model.\n",
    "\n",
    "\n",
    "\n",
    "Model validation is carried out after model training. Together with model training, model validation aims to find an optimal model with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[ 0.  0. ...,  0.  0.], n_iter=10, test_size=0.25, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x0000000009826BA8>)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'imp__strategy': ['mean', 'median', 'most_frequent'], 'clf__criterion': ['gini', 'entropy'], 'clf__max_depth': [3, 4, 5, 6, 7, 8, 9], 'kbest__k': [3, 4, 5, 6, 7, 8, 9], 'clf__presort': [True, False], 'clf__max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       scoring=make_scorer(recall_score), verbose=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model parameters to grid search object\n",
    "gridCV_object = GridSearchCV(estimator = Pipeline2, \n",
    "                             param_grid = param_grid2,\n",
    "                             scoring = score,\n",
    "                             cv = StratifiedShuffleSplit(labels_train, test_size=0.25,  n_iter=10))\n",
    "\n",
    "# train the model\n",
    "gridCV_object.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__criterion': 'entropy',\n",
       " 'clf__max_depth': 9,\n",
       " 'clf__max_features': 'log2',\n",
       " 'clf__presort': False,\n",
       " 'imp__strategy': 'median',\n",
       " 'kbest__k': 8}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridCV_object.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search gives us the above parameters. i.e \n",
    "\n",
    "Imputer strategy should be median.\n",
    "SelectKBest k should be 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=8, score_func=<function f_classif at 0x0000000009826BA8>)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=9,\n",
      "            max_features='log2', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.81343\tPrecision: 0.33351\tRecall: 0.30650\tF1: 0.31944\tF2: 0.31155\n",
      "\tTotal predictions: 14000\tTrue positives:  613\tFalse positives: 1225\tFalse negatives: 1387\tTrue negatives: 10775\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_final = Pipeline([\n",
    "        ('imp', Imputer(missing_values='NaN',strategy='median')),\n",
    "        ('kbest', SelectKBest(f_classif,k=8)),\n",
    "        ('clf', DecisionTreeClassifier(max_depth=9,max_features='log2',presort=False,criterion='entropy'))])\n",
    "\n",
    "test_classifier(clf_final, df1,features_list, folds= 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can clearly see that the tuned classifier gives us the better scores for each metrics.\n",
    "\n",
    "Below I have pasted the metrics for the classifier which is not tuned properly has a comparatively lower metrics to our tuned classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('imp', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.80379\tPrecision: 0.28571\tRecall: 0.24900\tF1: 0.26610\tF2: 0.25557\n",
      "\tTotal predictions: 14000\tTrue positives:  498\tFalse positives: 1245\tFalse negatives: 1502\tTrue negatives: 10755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf3, df1, features_list, folds = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Shrey",
   "language": "python",
   "name": "shrey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
